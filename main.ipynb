{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "941e1d2e",
   "metadata": {},
   "source": [
    "# <center> Topic modeling of scientific research papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf54df6",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc0fd4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Azus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Azus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Azus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Azus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a89fa3",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e38cceda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rotation Invariance Neural Network</td>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TITLE  \\\n",
       "ID                                                      \n",
       "1         Reconstructing Subject-Specific Effect Maps   \n",
       "2                  Rotation Invariance Neural Network   \n",
       "3   Spherical polyharmonics and Poisson kernels fo...   \n",
       "\n",
       "                                             ABSTRACT  Computer Science  \\\n",
       "ID                                                                        \n",
       "1     Predictive models allow subject-specific inf...                 1   \n",
       "2     Rotation invariance and translation invarian...                 1   \n",
       "3     We introduce and develop the notion of spher...                 0   \n",
       "\n",
       "    Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "ID                                                           \n",
       "1         0            0           0                     0   \n",
       "2         0            0           0                     0   \n",
       "3         0            1           0                     0   \n",
       "\n",
       "    Quantitative Finance  \n",
       "ID                        \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"data/train.csv\", index_col=\"ID\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf1501f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20972, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88215cd9",
   "metadata": {},
   "source": [
    "We have 20972 instances in our training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b3aef3",
   "metadata": {},
   "source": [
    "Now let's seperate the targets from the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fea901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll only use the Abstract since it generally contains more information than the title \n",
    "\n",
    "X=pd.DataFrame(df[\"ABSTRACT\"])  # So it still remains a dataframe\n",
    "\n",
    "y=df.drop([\"TITLE\", \"ABSTRACT\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b7af4e",
   "metadata": {},
   "source": [
    "### Cleaning (stopwords ..etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efdab79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_text(text):\n",
    "    clean = re.sub(\"\\n\",\" \",text)\n",
    "    clean=clean.lower()\n",
    "    clean=re.sub(r\"[~.,%/:;?_&+*=!-]\",\" \",clean)\n",
    "    clean=re.sub(\"[^a-z]\",\" \",clean)\n",
    "    clean=clean.lstrip()\n",
    "    clean=re.sub(\"\\s{2,}\",\" \",clean)\n",
    "    return clean\n",
    "\n",
    "X[\"cleaned_abstract\"]=X[\"ABSTRACT\"].apply(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24e1d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"cleaned_abstract\"] = X[\"cleaned_abstract\"].apply(lambda x: ' '.join([word for word in x.split() if len(word)>3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaeaa32",
   "metadata": {},
   "source": [
    "Now we will do the next step cleaning process which is stop word removal and then we will lemmatize the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2721c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=stopwords.words('english')\n",
    "stop.append(\"also\")\n",
    "X[\"stop_removed_abstract\"]=X[\"cleaned_abstract\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7570648e",
   "metadata": {},
   "source": [
    "Now to tokenize and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "611652c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X[\"tokenized\"]=X[\"stop_removed_abstract\"].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c4c6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_lemmatizer(text):\n",
    "    lem_text = [WordNetLemmatizer().lemmatize(i,pos='v') for i in text]\n",
    "    return lem_text\n",
    "\n",
    "X[\"lemmatized\"]=X[\"tokenized\"].apply(lambda x: word_lemmatizer(x))\n",
    "X[\"lemmatize_joined\"]=X[\"lemmatized\"].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f103c6e",
   "metadata": {},
   "source": [
    "Now we'll tokenize and create our BOW vector, however because of the large vocabulary in our dataset we'll only restrict our BOW to 3000 features (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83df2d35",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47aaa573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azus\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "counter = CountVectorizer(tokenizer=casual_tokenize, max_features=3000)\n",
    "bow_docs = pd.DataFrame(counter.fit_transform(raw_documents=X.lemmatize_joined).toarray(), columns=counter.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a46efa99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abelian</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absorb</th>\n",
       "      <th>absorption</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstraction</th>\n",
       "      <th>abundance</th>\n",
       "      <th>...</th>\n",
       "      <th>worst</th>\n",
       "      <th>would</th>\n",
       "      <th>write</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yield</th>\n",
       "      <th>young</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeta</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abelian  ability  able  absence  absolute  absorb  absorption  abstract  \\\n",
       "0        0        0     0        0         0       0           0         0   \n",
       "1        0        0     0        0         0       0           0         0   \n",
       "2        0        0     0        0         0       0           0         0   \n",
       "\n",
       "   abstraction  abundance  ...  worst  would  write  year  years  yield  \\\n",
       "0            0          0  ...      0      0      0     0      0      2   \n",
       "1            0          0  ...      0      0      0     0      0      0   \n",
       "2            0          0  ...      0      0      0     0      0      0   \n",
       "\n",
       "   young  zero  zeta  zone  \n",
       "0      0     0     0     0  \n",
       "1      0     0     0     0  \n",
       "2      0     0     0     0  \n",
       "\n",
       "[3 rows x 3000 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_docs.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf3aab",
   "metadata": {},
   "source": [
    "Now we'll train a Word2Vec model, using these BOW vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155c194d",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d980039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = X[\"lemmatize_joined\"].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05b4a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model= Word2Vec(tokens, min_count=60,vector_size=300,\n",
    "                     window=10,\n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007,\n",
    "                     workers = 4,\n",
    "                     seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d852316",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abelian</th>\n",
       "      <td>0.071531</td>\n",
       "      <td>-0.384007</td>\n",
       "      <td>-0.202899</td>\n",
       "      <td>-0.326325</td>\n",
       "      <td>0.111853</td>\n",
       "      <td>0.344678</td>\n",
       "      <td>0.414825</td>\n",
       "      <td>0.386427</td>\n",
       "      <td>-0.879540</td>\n",
       "      <td>0.736188</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.814025</td>\n",
       "      <td>0.145155</td>\n",
       "      <td>0.694547</td>\n",
       "      <td>-0.350781</td>\n",
       "      <td>-0.938837</td>\n",
       "      <td>0.108248</td>\n",
       "      <td>-0.011665</td>\n",
       "      <td>-0.033070</td>\n",
       "      <td>-0.012399</td>\n",
       "      <td>0.454002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability</th>\n",
       "      <td>-0.006661</td>\n",
       "      <td>0.807935</td>\n",
       "      <td>0.129141</td>\n",
       "      <td>-0.836222</td>\n",
       "      <td>-0.522488</td>\n",
       "      <td>-0.199422</td>\n",
       "      <td>0.475836</td>\n",
       "      <td>-0.122656</td>\n",
       "      <td>0.065119</td>\n",
       "      <td>0.015068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.413749</td>\n",
       "      <td>0.112707</td>\n",
       "      <td>-0.764179</td>\n",
       "      <td>-0.554890</td>\n",
       "      <td>0.103974</td>\n",
       "      <td>0.307676</td>\n",
       "      <td>-1.342367</td>\n",
       "      <td>-0.247328</td>\n",
       "      <td>0.759758</td>\n",
       "      <td>0.188017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>-0.696430</td>\n",
       "      <td>0.612448</td>\n",
       "      <td>0.177760</td>\n",
       "      <td>-0.369662</td>\n",
       "      <td>-0.144763</td>\n",
       "      <td>0.177897</td>\n",
       "      <td>-0.121177</td>\n",
       "      <td>0.084033</td>\n",
       "      <td>-0.000316</td>\n",
       "      <td>-0.214076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060553</td>\n",
       "      <td>0.349389</td>\n",
       "      <td>-0.575268</td>\n",
       "      <td>-0.313550</td>\n",
       "      <td>1.074376</td>\n",
       "      <td>0.203980</td>\n",
       "      <td>-0.264571</td>\n",
       "      <td>0.502461</td>\n",
       "      <td>0.512547</td>\n",
       "      <td>-0.079062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6    \\\n",
       "abelian  0.071531 -0.384007 -0.202899 -0.326325  0.111853  0.344678  0.414825   \n",
       "ability -0.006661  0.807935  0.129141 -0.836222 -0.522488 -0.199422  0.475836   \n",
       "able    -0.696430  0.612448  0.177760 -0.369662 -0.144763  0.177897 -0.121177   \n",
       "\n",
       "              7         8         9    ...       290       291       292  \\\n",
       "abelian  0.386427 -0.879540  0.736188  ... -0.814025  0.145155  0.694547   \n",
       "ability -0.122656  0.065119  0.015068  ... -0.413749  0.112707 -0.764179   \n",
       "able     0.084033 -0.000316 -0.214076  ...  0.060553  0.349389 -0.575268   \n",
       "\n",
       "              293       294       295       296       297       298       299  \n",
       "abelian -0.350781 -0.938837  0.108248 -0.011665 -0.033070 -0.012399  0.454002  \n",
       "ability -0.554890  0.103974  0.307676 -1.342367 -0.247328  0.759758  0.188017  \n",
       "able    -0.313550  1.074376  0.203980 -0.264571  0.502461  0.512547 -0.079062  \n",
       "\n",
       "[3 rows x 300 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = w2v_model.wv[bow_docs.columns]\n",
    "w2v = pd.DataFrame(w2v, index=bow_docs.columns)\n",
    "w2v.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3edd7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "docs=[]\n",
    "for i in X.lemmatized.index:\n",
    "    doc_vec=0\n",
    "    for j in range(0,len(X.lemmatized[i])):\n",
    "        if X.lemmatized[i][j] in w2v.index:\n",
    "            doc_vec=doc_vec+w2v.loc[X.lemmatized[i][j]].values\n",
    "            doc_vec = doc_vec.tolist()\n",
    "            \n",
    "    corpus['sent{}'.format(i)] =  doc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03e5bd0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>-3.062022</td>\n",
       "      <td>42.014184</td>\n",
       "      <td>-13.941606</td>\n",
       "      <td>18.081900</td>\n",
       "      <td>-41.600676</td>\n",
       "      <td>-8.736260</td>\n",
       "      <td>9.655307</td>\n",
       "      <td>-37.089094</td>\n",
       "      <td>-49.594600</td>\n",
       "      <td>-22.866710</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.243809</td>\n",
       "      <td>2.117328</td>\n",
       "      <td>-22.454697</td>\n",
       "      <td>-18.458828</td>\n",
       "      <td>30.521430</td>\n",
       "      <td>14.686189</td>\n",
       "      <td>28.393453</td>\n",
       "      <td>20.410135</td>\n",
       "      <td>-3.801300</td>\n",
       "      <td>18.568600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>6.600168</td>\n",
       "      <td>10.394864</td>\n",
       "      <td>7.854636</td>\n",
       "      <td>8.534307</td>\n",
       "      <td>-9.937889</td>\n",
       "      <td>-7.584500</td>\n",
       "      <td>14.686002</td>\n",
       "      <td>0.440937</td>\n",
       "      <td>6.303740</td>\n",
       "      <td>3.194464</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.172449</td>\n",
       "      <td>6.711353</td>\n",
       "      <td>-4.736738</td>\n",
       "      <td>-6.607900</td>\n",
       "      <td>-6.457617</td>\n",
       "      <td>2.236510</td>\n",
       "      <td>-1.530686</td>\n",
       "      <td>1.243183</td>\n",
       "      <td>-11.060441</td>\n",
       "      <td>7.263927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent3</th>\n",
       "      <td>4.507501</td>\n",
       "      <td>-0.835507</td>\n",
       "      <td>-0.655553</td>\n",
       "      <td>-16.620234</td>\n",
       "      <td>-12.343077</td>\n",
       "      <td>2.279687</td>\n",
       "      <td>7.447342</td>\n",
       "      <td>22.037397</td>\n",
       "      <td>-32.356278</td>\n",
       "      <td>14.367546</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.200364</td>\n",
       "      <td>-0.330609</td>\n",
       "      <td>5.144783</td>\n",
       "      <td>-5.258108</td>\n",
       "      <td>-14.352036</td>\n",
       "      <td>-10.115130</td>\n",
       "      <td>-0.953852</td>\n",
       "      <td>-1.778798</td>\n",
       "      <td>-15.192590</td>\n",
       "      <td>9.623202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1          2          3          4         5    \\\n",
       "sent1 -3.062022  42.014184 -13.941606  18.081900 -41.600676 -8.736260   \n",
       "sent2  6.600168  10.394864   7.854636   8.534307  -9.937889 -7.584500   \n",
       "sent3  4.507501  -0.835507  -0.655553 -16.620234 -12.343077  2.279687   \n",
       "\n",
       "             6          7          8          9    ...        290       291  \\\n",
       "sent1   9.655307 -37.089094 -49.594600 -22.866710  ...  -9.243809  2.117328   \n",
       "sent2  14.686002   0.440937   6.303740   3.194464  ... -15.172449  6.711353   \n",
       "sent3   7.447342  22.037397 -32.356278  14.367546  ...  -4.200364 -0.330609   \n",
       "\n",
       "             292        293        294        295        296        297  \\\n",
       "sent1 -22.454697 -18.458828  30.521430  14.686189  28.393453  20.410135   \n",
       "sent2  -4.736738  -6.607900  -6.457617   2.236510  -1.530686   1.243183   \n",
       "sent3   5.144783  -5.258108 -14.352036 -10.115130  -0.953852  -1.778798   \n",
       "\n",
       "             298        299  \n",
       "sent1  -3.801300  18.568600  \n",
       "sent2 -11.060441   7.263927  \n",
       "sent3 -15.192590   9.623202  \n",
       "\n",
       "[3 rows x 300 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vec = pd.DataFrame(corpus).T\n",
    "doc_vec.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00aafebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20972, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b87af",
   "metadata": {},
   "source": [
    "Now that we have our document vectors, we can go ahead and use the features to train the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f4154be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train , y_test = train_test_split(doc_vec, y.values, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469b138",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a423115",
   "metadata": {},
   "source": [
    "Now, because we have a **multi-label** classification problem, we'll use the **BinaryRelavance** function of the scikit-learn multi package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46a18014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "model_performance = pd.DataFrame(columns=['Accuracy','F-1 Score (micro)','F1-Score (weighted)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7a4ea",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e7ac9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Azus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Azus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Azus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Azus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.6481525625744935\n",
      "F1 score (micro) is  0.8036556603773585\n",
      "F1 score (weighted) is  0.8007981822974667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "lgr=BinaryRelevance(LogisticRegression())\n",
    "lgr.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = lgr.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(y_test,predictions)\n",
    "f1_micro=f1_score(y_test, predictions, average=\"micro\")\n",
    "f1_weighted=f1_score(y_test, predictions, average=\"weighted\")\n",
    "\n",
    "model_performance.loc['LogisticRegression'] = [acc, f1_micro, f1_weighted]\n",
    "\n",
    "print('Accuracy = ', acc)\n",
    "print('F1 score (micro) is ',f1_micro)\n",
    "print('F1 score (weighted) is ',f1_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01252039",
   "metadata": {},
   "source": [
    "## Random  Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9cdeda3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.6617401668653159\n",
      "F1 score (micro) is  0.8052990766760337\n",
      "F1 score (weighted) is  0.7964995411195676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf=BinaryRelevance(RandomForestClassifier())\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(y_test,predictions)\n",
    "f1_micro=f1_score(y_test, predictions, average=\"micro\")\n",
    "f1_weighted=f1_score(y_test, predictions, average=\"weighted\")\n",
    "\n",
    "model_performance.loc['RandomForestClassifier'] = [acc, f1_micro, f1_weighted]\n",
    "\n",
    "print('Accuracy = ', acc)\n",
    "print('F1 score (micro) is ',f1_micro)\n",
    "print('F1 score (weighted) is ',f1_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51040eeb",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3a67ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.6624553039332539\n",
      "F1 score (micro) is  0.81234183375511\n",
      "F1 score (weighted) is  0.8090999527813589\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "rf=BinaryRelevance(XGBClassifier())\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(y_test,predictions)\n",
    "f1_micro=f1_score(y_test, predictions, average=\"micro\")\n",
    "f1_weighted=f1_score(y_test, predictions, average=\"weighted\")\n",
    "\n",
    "model_performance.loc['XGBClassifier'] = [acc, f1_micro, f1_weighted]\n",
    "\n",
    "print('Accuracy = ', acc)\n",
    "print('F1 score (micro) is ',f1_micro)\n",
    "print('F1 score (weighted) is ',f1_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491af463",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c518d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5a51f_row0_col0, #T_5a51f_row0_col1, #T_5a51f_row1_col2 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5a51f_row0_col2 {\n",
       "  background-color: #adc9fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5a51f_row1_col0 {\n",
       "  background-color: #c53334;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5a51f_row1_col1 {\n",
       "  background-color: #779af7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5a51f_row2_col0, #T_5a51f_row2_col1, #T_5a51f_row2_col2 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5a51f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5a51f_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_5a51f_level0_col1\" class=\"col_heading level0 col1\" >F-1 Score (micro)</th>\n",
       "      <th id=\"T_5a51f_level0_col2\" class=\"col_heading level0 col2\" >F1-Score (weighted)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5a51f_level0_row0\" class=\"row_heading level0 row0\" >LogisticRegression</th>\n",
       "      <td id=\"T_5a51f_row0_col0\" class=\"data row0 col0\" >64.82%</td>\n",
       "      <td id=\"T_5a51f_row0_col1\" class=\"data row0 col1\" >0.803656</td>\n",
       "      <td id=\"T_5a51f_row0_col2\" class=\"data row0 col2\" >80.08%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a51f_level0_row1\" class=\"row_heading level0 row1\" >RandomForestClassifier</th>\n",
       "      <td id=\"T_5a51f_row1_col0\" class=\"data row1 col0\" >66.17%</td>\n",
       "      <td id=\"T_5a51f_row1_col1\" class=\"data row1 col1\" >0.805299</td>\n",
       "      <td id=\"T_5a51f_row1_col2\" class=\"data row1 col2\" >79.65%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a51f_level0_row2\" class=\"row_heading level0 row2\" >XGBClassifier</th>\n",
       "      <td id=\"T_5a51f_row2_col0\" class=\"data row2 col0\" >66.25%</td>\n",
       "      <td id=\"T_5a51f_row2_col1\" class=\"data row2 col1\" >0.812342</td>\n",
       "      <td id=\"T_5a51f_row2_col2\" class=\"data row2 col2\" >80.91%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e67e86dbe0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance.style.background_gradient(cmap='coolwarm').format({'Accuracy': '{:.2%}',\n",
    "                                                                     'F1-Score (micro)': '{:.2%}',\n",
    "                                                                     'F1-Score (weighted)': '{:.2%}',\n",
    "                                                                     })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa5482d63cb5f988d9583ee6972df1d00efe8695a573a74b329603e727d7c156"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
